# Use RHEL 9 as the base image
FROM registry.access.redhat.com/ubi9/ubi:latest

# Set environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
ENV SPARK_HOME=/opt/spark
ENV DATABUCK_HOME=/opt/databuck
ENV DATABUCK_LOG=/opt/databuck/logs
ENV TOMCAT_HOME=/usr/share/tomcat
ENV PATH="$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH"

# Install required dependencies
RUN yum update -y && \
    yum install -y \
    java-1.8.0-openjdk \
    wget \
    tar \
    mysql-server \
    tomcat \
    unzip && \
    yum clean all

# Create required directories
RUN mkdir -p /opt/databuck/logs/Validation && \
    mkdir -p /opt/databuck/logs/Template && \
    mkdir -p /opt/databuck/logs/GlobalFilters && \
    mkdir -p /opt/databuck/logs/validatecustomrule && \
    mkdir -p /opt/databuck/logs/Other && \
    mkdir -p /opt/temp

# Download and extract Databuck
WORKDIR /opt/temp
RUN wget https://tmplog1.s3.amazonaws.com/Databuck/Databuck-Artifacts-core.tar.gz && \
    tar -xvf Databuck-Artifacts-core.tar.gz && \
    cp -r databuck-engine.jar password-encryptor.jar propertiesFiles scripts $DATABUCK_HOME

# Install and configure MySQL
RUN systemctl start mysqld && \
    mysql -u root -e "ALTER USER 'root'@'localhost' IDENTIFIED BY 'Databuck125!';" && \
    mysql -u root -pDatabuck125! -e "CREATE USER 'databuck'@'localhost' IDENTIFIED BY 'mysql0501';" && \
    mysql -u root -pDatabuck125! -e "CREATE DATABASE databuck_app_db;" && \
    mysql -u root -pDatabuck125! -e "CREATE DATABASE databuck_results_db;" && \
    mysql -u root -pDatabuck125! -e "GRANT ALL PRIVILEGES ON databuck_app_db.* TO 'databuck'@'localhost';" && \
    mysql -u root -pDatabuck125! -e "GRANT ALL PRIVILEGES ON databuck_results_db.* TO 'databuck'@'localhost';" && \
    mysql -u root -pDatabuck125! -e "FLUSH PRIVILEGES;" && \
    systemctl enable mysqld

# Install and configure Tomcat
RUN systemctl enable tomcat && \
    systemctl start tomcat

# Download and extract Spark
WORKDIR /opt
RUN wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz && \
    tar -xvf spark-2.4.8-bin-hadoop2.7.tgz && \
    mv spark-2.4.8-bin-hadoop2.7 spark

# Copy Databuck properties files (Update credentials inside these files if needed)
COPY appdb.properties $DATABUCK_HOME/propertiesFiles/appdb.properties
COPY resultsdb.properties $DATABUCK_HOME/propertiesFiles/resultsdb.properties

# Set working directory
WORKDIR /opt/databuck

# Expose necessary ports
EXPOSE 8080 3306 7077

# Start services
CMD ["/bin/bash"]
